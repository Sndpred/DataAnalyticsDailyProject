{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dfa67d-ca1b-4f46-94cb-7797275dded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HariBahadur', 'loves', 'Python']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sndpred/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"HariBahadur loves Python\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab0e1eb-e2a8-4669-aaa1-2c85562c7c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['the game unfolds on a diamond-shaped field with four bases: first, second, third, and home plate.', 'Two teams of nine players each take turns batting and fielding.', 'The objective for the batting team is to hit the ball thrown by the pitcher and advance around the bases, ultimately touching home plate to score a run.', 'The fielding team aims to get three outs by striking out batters, tagging runners, or catching batted balls, at which point the teams switch roles.', 'A standard game consists of nine innings, and the team with the most runs at the end wins.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenization splits a text into sentences.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "#text = \"I love programming. It's fun!\"\n",
    "\n",
    "text = \"the game unfolds on a diamond-shaped field with four bases: first, second, third, and home plate. Two teams of nine players each take turns batting and fielding. The objective for the batting team is to hit the ball thrown by the pitcher and advance around the bases, ultimately touching home plate to score a run. The fielding team aims to get three outs by striking out batters, tagging runners, or catching batted balls, at which point the teams switch roles. A standard game consists of nine innings, and the team with the most runs at the end wins.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a6046f-8566-481e-ab16-985bdda39b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization: ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "#Character tokenization splits text into individual characters.\n",
    "# Example word\n",
    "word = \"programming\"\n",
    "# Character Tokenization (manual splitting)\n",
    "char_tokens = list(word)\n",
    "# Print the result\n",
    "print(\"Character Tokenization:\", char_tokens)\n",
    "Character_Tokenization: ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i',\n",
    "'n', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5fa6a0-c225-4ffe-b865-3ffb15a4db16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['I', 'love', 'programming.']\n"
     ]
    }
   ],
   "source": [
    "#Whitespace tokenization splits the text based on spaces (whitespace characters).\n",
    "# Example sentence\n",
    "text = \"I love programming.\"\n",
    "# Whitespace Tokenization (splitting based on space)\n",
    "whitespace_tokens = text.split( )\n",
    "# Print the result\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf090f8-06ba-418f-90fe-658f09115c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-Aware Tokenization: ['Hello', '!', 'How', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "#Punctuation-Aware Tokenization handles punctuation separately from words.\n",
    "# Example sentence with punctuation\n",
    "text = \"Hello! How are you doing?\"\n",
    "# Word Tokenization (with punctuation handling)\n",
    "word_tokens_with_punct = word_tokenize(text)\n",
    "# Print the result\n",
    "print(\"Punctuation-Aware Tokenization:\", word_tokens_with_punct)\n",
    "Punctuation_Aware_Tokenization: ['Hello', '!', 'How', 'are', 'you',\n",
    "'doing', '?']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e722f17-3a16-4bab-8334-a705a1cd9c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sndpred/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Example sentence (tokens)\n",
    "tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "tokens\n",
    "['I', 'love', 'programming', 'and', 'it', 'is', 'fun']\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db3f0eb8-da5a-4045-b010-3494d67fe4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccda0c0f-7914-42c1-959b-f091c99e9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words List: {\"they're\", 'd', 'shan', 'yourself', \"weren't\", \"i've\", \"he'll\", 'll', \"doesn't\", 'only', 'there', 'on', 'but', 'doing', 'it', \"he'd\", 'again', \"haven't\", 'be', \"we've\", 'our', 'after', 'any', 'her', 'having', 'hasn', 'needn', 'won', 'once', 'further', \"needn't\", 'being', 'these', 'before', 'and', 'few', 'they', 'i', 'o', 'its', 'couldn', 'theirs', 'how', 'down', 'own', 'mightn', 'their', \"i'd\", 'or', 'do', 's', 'same', \"shouldn't\", 'until', 'will', 'just', 'other', 'why', 'y', \"it'd\", 'nor', \"that'll\", \"they'll\", \"she'll\", 'than', 'this', 'have', 'was', 'should', \"wasn't\", 'has', 'during', 'does', 'all', 'doesn', 'very', 'did', 'an', 'by', 'your', 'aren', 'so', 'm', \"she's\", 'too', 'about', 'here', 'in', 'yourselves', 'himself', 'didn', 'isn', \"it's\", 'you', \"don't\", 'been', 'above', 'through', \"won't\", 'am', \"it'll\", 'off', \"you're\", 'out', \"you'll\", \"hasn't\", 'no', 'shouldn', 'had', \"he's\", 'hers', 'we', 'haven', 'such', 'herself', 'when', 'ma', 'them', 'ours', 'my', \"couldn't\", 're', 'is', \"wouldn't\", 'his', 'under', \"we'll\", 'if', 'for', 'wasn', 'up', 'itself', \"they'd\", 'he', 'which', 'against', 'below', \"i'll\", 'she', 'as', \"she'd\", 'both', 'because', 'a', \"you've\", \"you'd\", \"mustn't\", 'each', 'that', 'whom', 'can', 'ourselves', 'what', 'between', \"shan't\", 'more', 'where', 't', 'then', \"isn't\", \"mightn't\", 'weren', 'are', \"aren't\", 'from', 'him', 'at', 'don', 've', \"we'd\", 'most', 'while', 'mustn', 'those', 'over', \"i'm\", \"didn't\", 'the', 'who', 'me', 'myself', \"we're\", 'into', 'themselves', 'not', \"should've\", 'to', 'some', \"they've\", 'yours', \"hadn't\", 'were', 'with', 'now', 'hadn', 'ain', 'of', 'wouldn'}\n"
     ]
    }
   ],
   "source": [
    "# Print the stop words list\n",
    "print(\"Stop Words List:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55422e0-480d-4bff-b179-82a03e24cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sndpred/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7038304d-67cc-46b0-956b-9ce7f38db2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Word Removal: ['love', 'programming', 'fun']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in\n",
    "stop_words]\n",
    "# Print the filtered tokens\n",
    "#tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "print(\"Tokens after Stop Word Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2216b7d-a2bf-4075-b377-1214e56d59ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "#List comprehension\n",
    "#new_list = [expression for item in iterable if condition]\n",
    "\n",
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = []\n",
    "\n",
    "for fruit in basket:\n",
    "    if len(fruit) > 5:\n",
    "        long_fruits.append(fruit)\n",
    "        \n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "385adbb2-d65a-43ae-b808-e04b4c4ed066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# With List Comprehension\n",
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = [fruit for fruit in basket if len(fruit) > 5]\n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45be63da-c81e-4e01-b1c0-5fda30d89060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# another example\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "doubled = [num * 2 for num in numbers]\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc1ec452-ddbb-437e-9f24-8a18fdbcae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', 'o', 'v', 'e'], ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g'], ['f', 'u', 'n']]\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuation. Punctuation can often be noise in text processing tasks, so we will remove them.\n",
    "import string\n",
    "# Removing punctuation\n",
    "reviews_no_punct = [[word for word in tokens if word not in\n",
    "string.punctuation] for tokens in filtered_tokens]\n",
    "print(reviews_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7181940-bc7f-404d-9e54-cc5bba00eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', 'o', 'v', 'e'], ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g'], ['f', 'u', 'n']]\n"
     ]
    }
   ],
   "source": [
    "#Removing Special Characters Special characters such as “#”, “@”, or emoji may need to be removed or treated based on their relevance.\n",
    "import re\n",
    "# Removing special characters\n",
    "def remove_special_chars(tokens):\n",
    "    return [re.sub(r'[^A-Za-z0-9]+', '', word) for word in tokens]\n",
    "reviews_cleaned = [remove_special_chars(tokens) for tokens in filtered_tokens]\n",
    "print(reviews_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17619806-19d4-44ee-9fb0-69bd47467431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!', 'How', 'are', 'you', 'doing', '?']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # Stemming algorithm\n",
    "from nltk.tokenize import word_tokenize # For breaking text into\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f8a49b5-e9ac-4693-b266-6b3ae23f6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Porter Stemmer is a popular algorithm for stemming in English.\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "901270c8-b5db-4bb0-9dc8-711dcf3b94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "text = \"I am loving the process of learning and understanding NLP concepts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c285d87b-32b5-4e37-908f-0c311a5b0fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'am', 'loving', 'the', 'process', 'of', 'learning', 'and', 'understanding', 'NLP', 'concepts', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Text\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92e158f2-1389-44e7-bac0-a91654db567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['i', 'am', 'love', 'the', 'process', 'of', 'learn', 'and', 'understand', 'nlp', 'concept', '.']\n"
     ]
    }
   ],
   "source": [
    "# apply stemming to each word in the list of tokens\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3ee6b-fa20-4682-a66f-400e185e44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda74)",
   "language": "python",
   "name": "bda74"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
