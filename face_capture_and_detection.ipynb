{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce52682f-967c-4572-86e0-256010b448ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install \"numpy<2\"\n",
    "\n",
    "\n",
    "# Import essential libraries in the notebook\n",
    " \n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2dc4da-d43f-4b14-bb8b-9067692a4beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to save the image, 'q' to quit.\n",
      "Saved: ./data/person1/person1_0.jpg\n",
      "Saved: ./data/person1/person1_1.jpg\n",
      "Saved: ./data/person1/person1_2.jpg\n",
      "Saved: ./data/person1/person1_3.jpg\n",
      "Saved: ./data/person1/person1_4.jpg\n",
      "Using image: ./data/person1/person1_4.jpg\n"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam using OpenCV\n",
    " \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Capture frames\n",
    " \n",
    "ret, frame = cap.read()# code for capturing and saving images\n",
    " \n",
    "import cv2\n",
    "\n",
    "import os\n",
    " \n",
    "# Create directories for each person (label)\n",
    "\n",
    "label = \"person1\"  # Change this for different classes\n",
    "\n",
    "output_dir = f\"./data/{label}\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    " \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 's' to save the image, 'q' to quit.\")\n",
    " \n",
    "image_count = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('s'):  # Save the frame\n",
    "\n",
    "        image_path = os.path.join(output_dir, f\"{label}_{image_count}.jpg\")\n",
    "\n",
    "        cv2.imwrite(image_path, frame)\n",
    "\n",
    "        image_count += 1\n",
    "\n",
    "        print(f\"Saved: {image_path}\")\n",
    "\n",
    "    elif key == ord('q'):  # Quit the capture\n",
    "\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    " \n",
    "# Path to the directory where images are saved\n",
    "\n",
    "image_dir = \"./data/person1\"\n",
    " \n",
    "# Get the latest saved image from the directory\n",
    "\n",
    "images = sorted(os.listdir(image_dir), key=lambda x: os.path.getctime(os.path.join(image_dir, x)))\n",
    "\n",
    "if len(images) == 0:\n",
    "\n",
    "    print(\"No images found in the directory.\")\n",
    "\n",
    "    exit()\n",
    " \n",
    "latest_image_path = os.path.join(image_dir, images[-1])\n",
    "\n",
    "print(f\"Using image: {latest_image_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b428905-e0be-4af4-8f85-6ba9ae4a837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the latest image\n",
    "\n",
    "image = cv2.imread(latest_image_path)\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# Load the Haar Cascade face detection model using OpenCV.\n",
    "\n",
    "# This model will be used to detect faces in the captured images.\n",
    " \n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74317ba8-40e3-49f8-81a3-d0b880b5f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea946a2-46f6-4144-a361-eef78bc83104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection with adjusted parameters\n",
    "\n",
    "faces = face_cascade.detectMultiScale(\n",
    "\n",
    "    gray,\n",
    "\n",
    "    scaleFactor=1.05,  # Reduce the scale step for finer detection\n",
    "\n",
    "    minNeighbors=6,    # Increase for stricter face detection\n",
    "\n",
    "    minSize=(50, 50)   # Minimum face size to detect\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2cecf3-04a9-4065-94c1-40a47af07a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes around detected faces\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    " \n",
    "# Display the image with bounding boxes\n",
    "\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87135c9d-4b39-44f6-8c7c-ca6e2b0a552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 81, 105, 125],\n",
       "        [ 80, 104, 124],\n",
       "        [ 79, 103, 123],\n",
       "        ...,\n",
       "        [162, 183, 211],\n",
       "        [160, 181, 209],\n",
       "        [161, 182, 210]],\n",
       "\n",
       "       [[ 81, 105, 125],\n",
       "        [ 81, 105, 125],\n",
       "        [ 80, 104, 124],\n",
       "        ...,\n",
       "        [162, 183, 211],\n",
       "        [161, 182, 210],\n",
       "        [161, 182, 210]],\n",
       "\n",
       "       [[ 81, 105, 125],\n",
       "        [ 81, 105, 125],\n",
       "        [ 81, 105, 125],\n",
       "        ...,\n",
       "        [163, 184, 212],\n",
       "        [161, 182, 210],\n",
       "        [161, 182, 210]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[146, 167, 194],\n",
       "        [146, 167, 194],\n",
       "        [146, 167, 194],\n",
       "        ...,\n",
       "        [137, 161, 191],\n",
       "        [135, 160, 192],\n",
       "        [136, 161, 193]],\n",
       "\n",
       "       [[145, 166, 193],\n",
       "        [145, 166, 193],\n",
       "        [146, 167, 194],\n",
       "        ...,\n",
       "        [138, 162, 192],\n",
       "        [136, 161, 193],\n",
       "        [136, 161, 193]],\n",
       "\n",
       "       [[145, 166, 193],\n",
       "        [145, 166, 193],\n",
       "        [146, 167, 194],\n",
       "        ...,\n",
       "        [138, 162, 192],\n",
       "        [136, 161, 193],\n",
       "        [136, 161, 193]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw bounding boxes and add labels\n",
    "\n",
    "if len(faces) == 0:\n",
    "\n",
    "    print(\"No faces detected.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # Draw rectangle with custom color and thickness\n",
    "\n",
    "        color = (0, 255, 0)  # Green color for the box\n",
    "\n",
    "        thickness = 2\n",
    "\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "\n",
    " # Add label above the rectangle\n",
    "\n",
    "label = \"Face\"\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "font_scale = 0.5\n",
    "\n",
    "font_thickness = 1\n",
    "\n",
    "label_size, _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "\n",
    "label_x = x\n",
    "\n",
    "label_y = y - 10 if y - 10 > 10 else y + 10  # Adjust position if label goes out of bounds\n",
    "\n",
    "cv2.rectangle(image, (label_x, label_y - label_size[1] - 2), (label_x + label_size[0], label_y + 2), color, cv2.FILLED)\n",
    "\n",
    "cv2.putText(image, label, (label_x, label_y), font, font_scale, (0, 0, 0), font_thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9772312-f187-4cfd-9769-59addc106645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe6be9-22bd-4100-9960-4ccf33bd024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with bounding boxes and labels\n",
    "\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdc74c-b845-4d34-a62d-b53f80971aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
